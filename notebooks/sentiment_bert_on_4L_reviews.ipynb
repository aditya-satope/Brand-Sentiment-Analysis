{"cells":[{"metadata":{"id":"EXiqLYc9XEhd","outputId":"beca31db-76c4-470d-d68f-874a29f2e075","trusted":true},"cell_type":"code","source":"!pip install transformers","execution_count":13,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.12)\nRequirement already satisfied: tokenizers==0.9.4 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.9.4)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.55.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.25.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.11.13)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (3.3.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from transformers) (20.8)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.4.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.4.0)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.12.5)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.2)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.0.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.15.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.2)\n","name":"stdout"}]},{"metadata":{"id":"C-Vy8RKJ42NO","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport time\nimport pickle\n\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nfrom sklearn.decomposition import PCA\n\nimport re\n\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom transformers import DistilBertTokenizer, DistilBertModel,AdamW, get_linear_schedule_with_warmup","execution_count":14,"outputs":[]},{"metadata":{"id":"Rpnchv4yXoqz","outputId":"bf20861c-47b1-480f-ec0d-66af98e7d4f1","trusted":true},"cell_type":"code","source":"if torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":15,"outputs":[{"output_type":"stream","text":"No GPU available, using the CPU instead.\n","name":"stdout"}]},{"metadata":{"id":"hAuKrs29zi9u","trusted":true},"cell_type":"code","source":"def set_seed(seed_value=42):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n","execution_count":16,"outputs":[]},{"metadata":{"id":"kzU73Wwincv1","trusted":true},"cell_type":"code","source":"model_name = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(model_name)\n\nmBERT = DistilBertModel.from_pretrained(model_name,\n                                  output_hidden_states = True,\n                                  output_attentions = True)","execution_count":17,"outputs":[]},{"metadata":{"id":"NIqgMa-mmI1g","trusted":true},"cell_type":"code","source":"amazon_reviews_path = \"../input/amazon-reviews-unlocked-mobile-phones/Amazon_Unlocked_Mobile.csv\"","execution_count":18,"outputs":[]},{"metadata":{"id":"P0ueh59I-7-h","outputId":"66115880-f53c-461f-f078-da86bff62e23","trusted":true},"cell_type":"code","source":"df = pd.read_csv(amazon_reviews_path)\ndf = df.sample(frac = 1.0,random_state = 42).reset_index(drop = True)\ndf.head()","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"                                        Product Name Brand Name   Price  \\\n0  Apple iPhone 6 Plus 128GB Factory Unlocked GSM...        NaN  699.95   \n1  HTC Desire 816 Dual Sim Unlocked Smartphone (W...        HTC  221.00   \n2      BLU Studio 5.0 C HD Unlocked Cellphone, Black        BLU  173.44   \n3                 Apple iPhone 5c 16GB (Pink) - AT&T      Apple  519.00   \n4  BLU PURE XL Smartphone - 4G LTE GSM Unlocked -...        BLU  129.99   \n\n   Rating                                            Reviews  Review Votes  \n0       1  Defective phone. Works only for two days. Phon...           5.0  \n1       3                                               Cool           0.0  \n2       5                    Beautiful and excellent quality           1.0  \n3       2  Gave 3 stars because it did not come with a ch...           0.0  \n4       1  High resolution camera so you can zoom in afte...           1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product Name</th>\n      <th>Brand Name</th>\n      <th>Price</th>\n      <th>Rating</th>\n      <th>Reviews</th>\n      <th>Review Votes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Apple iPhone 6 Plus 128GB Factory Unlocked GSM...</td>\n      <td>NaN</td>\n      <td>699.95</td>\n      <td>1</td>\n      <td>Defective phone. Works only for two days. Phon...</td>\n      <td>5.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>HTC Desire 816 Dual Sim Unlocked Smartphone (W...</td>\n      <td>HTC</td>\n      <td>221.00</td>\n      <td>3</td>\n      <td>Cool</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>BLU Studio 5.0 C HD Unlocked Cellphone, Black</td>\n      <td>BLU</td>\n      <td>173.44</td>\n      <td>5</td>\n      <td>Beautiful and excellent quality</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Apple iPhone 5c 16GB (Pink) - AT&amp;T</td>\n      <td>Apple</td>\n      <td>519.00</td>\n      <td>2</td>\n      <td>Gave 3 stars because it did not come with a ch...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>BLU PURE XL Smartphone - 4G LTE GSM Unlocked -...</td>\n      <td>BLU</td>\n      <td>129.99</td>\n      <td>1</td>\n      <td>High resolution camera so you can zoom in afte...</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"id":"zr1ImHIIGRxL","outputId":"f041a621-2759-418b-aed6-5e982b7965b1","trusted":true},"cell_type":"code","source":"print(len(df))","execution_count":20,"outputs":[{"output_type":"stream","text":"413840\n","name":"stdout"}]},{"metadata":{"id":"lz-Fpi8mFhUG","trusted":true},"cell_type":"code","source":"df_v1 = df.iloc[20000:]\ndf_v1.head()","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"                                            Product Name Brand Name   Price  \\\n20000  Samsung Galaxy Note 3 N900 32GB Unlocked GSM 4...    Samsung  249.99   \n20001  Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...        NaN  244.95   \n20002  Apple iPhone 5C Green 8GB Unlocked GSM Smartph...        NaN  159.99   \n20003           Apple iPhone 5c 32GB - Unlocked - (Blue)      Apple  224.77   \n20004  UHAPPY Touch Screen 5\" QHD MTK6582 Dual SIM Qu...     Wmicro   69.99   \n\n       Rating                                            Reviews  Review Votes  \n20000       4  Where is a set of instructions for us 'older u...           1.0  \n20001       5                                        Great phone           0.0  \n20002       3  Wouldnt know my daughter lost it 3days after p...           0.0  \n20003       5  The phone arrived on time,I'm happy with the p...           2.0  \n20004       4  Its a great phone, so far no problems. Fast to...           7.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Product Name</th>\n      <th>Brand Name</th>\n      <th>Price</th>\n      <th>Rating</th>\n      <th>Reviews</th>\n      <th>Review Votes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20000</th>\n      <td>Samsung Galaxy Note 3 N900 32GB Unlocked GSM 4...</td>\n      <td>Samsung</td>\n      <td>249.99</td>\n      <td>4</td>\n      <td>Where is a set of instructions for us 'older u...</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>20001</th>\n      <td>Sony XPERIA Z2 D6503 FACTORY UNLOCKED Internat...</td>\n      <td>NaN</td>\n      <td>244.95</td>\n      <td>5</td>\n      <td>Great phone</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20002</th>\n      <td>Apple iPhone 5C Green 8GB Unlocked GSM Smartph...</td>\n      <td>NaN</td>\n      <td>159.99</td>\n      <td>3</td>\n      <td>Wouldnt know my daughter lost it 3days after p...</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20003</th>\n      <td>Apple iPhone 5c 32GB - Unlocked - (Blue)</td>\n      <td>Apple</td>\n      <td>224.77</td>\n      <td>5</td>\n      <td>The phone arrived on time,I'm happy with the p...</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>20004</th>\n      <td>UHAPPY Touch Screen 5\" QHD MTK6582 Dual SIM Qu...</td>\n      <td>Wmicro</td>\n      <td>69.99</td>\n      <td>4</td>\n      <td>Its a great phone, so far no problems. Fast to...</td>\n      <td>7.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# neutral = df_v1[df[\"sentiment\"] == 1]\n# positive = df_v1[df[\"sentiment\"] == 2][:700]\n# negative = df_v1[df[\"sentiment\"] == 0][:1800]","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df_major_n = positive.copy()\n# df_major_n = df_major_n.append(neutral,ignore_index = True)\n# df_major_n = df_major_n.append(negative,ignore_index = True)\n# df_major_n = df_major_n.sample(frac = 1.0).reset_index(drop = True)\n# df_major_n.head(15)","execution_count":23,"outputs":[]},{"metadata":{"id":"HnaFAwyyAjGl","trusted":true},"cell_type":"code","source":"X = df[\"Reviews\"].astype(str)\ny = df[\"Rating\"]","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.describe()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"count     413840\nunique    162492\ntop         Good\nfreq        2879\nName: Reviews, dtype: object"},"metadata":{}}]},{"metadata":{"id":"dLCn4J_tB2vv","trusted":true},"cell_type":"code","source":"def project(targets):\n    L = []\n    targets = np.asarray(targets)\n    for i in range(len(targets)):\n        if targets[i] > 4:\n            L.append(2)\n        elif targets[i] < 2:\n            L.append(0)\n        else:\n            L.append(1)\n    return pd.Series(L)","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = project(y)","execution_count":27,"outputs":[]},{"metadata":{"id":"R_XRGwVzJ7d-","trusted":true},"cell_type":"code","source":"X_tr,X_val,y_tr,y_val = train_test_split(X,y,test_size = 0.30)\nX_val,X_te,y_val,y_te = train_test_split(X_val,y_val,test_size = 0.40)\nassert(X_tr.shape[0] == y_tr.shape[0])","execution_count":28,"outputs":[]},{"metadata":{"id":"4gJ_islGWwax","trusted":true},"cell_type":"code","source":"MAX_LEN = 200\ndef preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in tqdm(data):\n        #print(type(sent))\n        encoded_sent = tokenizer(\n            text=sent,\n            #add_special_tokens = True,\n            max_length=MAX_LEN,             \n            padding = 'max_length',\n            return_tensors='pt',           \n            return_attention_mask=True,\n            truncation = True   \n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    #print(type(input_ids))\n    input_ids = torch.cat(input_ids,dim=0)\n    attention_masks = torch.cat(attention_masks,dim=0)\n\n    return input_ids, attention_masks","execution_count":29,"outputs":[]},{"metadata":{"id":"oGROYPj4Z4lA","outputId":"d3603e7d-d77a-4726-cd58-05c6bc37093d","trusted":true,"collapsed":true},"cell_type":"code","source":"train_inputs, train_masks = preprocessing_for_bert(X_tr)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","execution_count":30,"outputs":[{"output_type":"stream","text":"  6%|▋         | 18156/289688 [00:29<07:23, 611.81it/s]\n","name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-81884368d99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing_for_bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-1ed799c2a789>\u001b[0m in \u001b[0;36mpreprocessing_for_bert\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mtruncation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             )\n\u001b[1;32m     18\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_sent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2354\u001b[0m                 \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m             )\n\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2424\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m         )\n\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    442\u001b[0m             )\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mfirst_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0msecond_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtext_pair\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mget_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mget_input_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    337\u001b[0m                     (\n\u001b[1;32m    338\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     )\n\u001b[1;32m    341\u001b[0m                 )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    337\u001b[0m                     (\n\u001b[1;32m    338\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                     )\n\u001b[1;32m    341\u001b[0m                 )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Checks whether `char` is a control character.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# These are technically control characters but we count them as whitespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"1dF2zKrzaQJN","trusted":true,"collapsed":true},"cell_type":"code","source":"\ntrain_labels = torch.tensor(y_tr.values)\nval_labels = torch.tensor(y_val.values)\n\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","execution_count":31,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_inputs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-d889f033b619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the DataLoader for our training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtrain_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_sampler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"]}]},{"metadata":{"id":"wHyIJo6Zau-j","trusted":true},"cell_type":"code","source":"# Create the BertClassfier class\n\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        \n        D_in, H, D_out = 768, 20, 3    # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        self.bert = mBERT\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Linear(H, D_out)\n        )\n        if freeze_bert:                 # Freeze the BERT model\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)     # Feed input to BERT\n        \n        #print(outputs[0].size())\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        # Feed input to classifier to compute logits\n        \n        logits = self.classifier(last_hidden_state_cls)\n        return logits","execution_count":null,"outputs":[]},{"metadata":{"id":"ezOdylqTbG0b","trusted":true},"cell_type":"code","source":"def initialize_model(epochs=1):\n    \n    bert_classifier = BertClassifier(freeze_bert=False)\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=1e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","execution_count":null,"outputs":[]},{"metadata":{"id":"C66HfUiwbMXo","trusted":true},"cell_type":"code","source":"\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            #print(step)\n            \n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training and validation results\n                #val_loss, val_accuracy = evaluate(model, val_dataloader)\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^9} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_inputs.size(),train_masks.size())","execution_count":null,"outputs":[]},{"metadata":{"id":"A4gJSkARbYmg","outputId":"cea3803e-3d86-405a-c6a9-b8747bbc7d16","trusted":true},"cell_type":"code","source":"set_seed()\nbert_classifier, optimizer, scheduler = initialize_model(epochs=1)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)","execution_count":null,"outputs":[]},{"metadata":{"id":"kk5WAYwDSm-o","outputId":"a15ab64b-9c36-49d4-e741-1dcdd448a1bd","trusted":true},"cell_type":"code","source":"train_loss, train_acc = evaluate(bert_classifier,train_dataloader)\nprint(\"Train Loss : {0:.5f}\\nTrain Accuracy : {1:.2f}\".format(train_loss,train_acc))\nval_loss, val_acc = evaluate(bert_classifier,val_dataloader)\nprint(\"Validation Loss : {0:.5f}\\nValidation Accuracy : {1:.2f}\".format(train_loss,train_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = {\"model\" : bert_classifier.state_dict(),\n              \"optimizer\" : optimizer.state_dict(),\n              \"scheduler\" : scheduler.state_dict(),\n              \"val_loss\" : val_loss,\n              \"val_acc\" : val_acc,\n              }\nPATH = \"distilbert_sentiment.pt\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(checkpoint,PATH)","execution_count":null,"outputs":[]},{"metadata":{"id":"EgRii7CVbc4O","trusted":true},"cell_type":"code","source":"def bert_predict(model, test_dataloader):\n    model.eval()\n\n    all_logits = []\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu()\n\n    return probs","execution_count":null,"outputs":[]},{"metadata":{"id":"9Sa0uiH9-nI-","trusted":true},"cell_type":"code","source":"def evaluate_thru_pipeline(model,X,y):\n    model.eval()\n    \n    labels = torch.tensor(project(y))\n    X_tokenized, X_masks = preprocessing_for_bert(X)\n\n    piped_ds = TensorDataset(X_tokenized, X_masks, labels)\n    piped_dl = DataLoader(piped_ds,batch_size = 16)\n    \n    #Get the probabilities\n    probs = bert_predict(model, piped_dl)\n\n    # Get the predictions\n    preds = torch.argmax(probs, dim=1).flatten()\n    \n    # Calculate the accuracy rate\n    accuracy = (preds == labels).cpu().numpy().mean() * 100\n\n    return preds,probs,accuracy","execution_count":null,"outputs":[]},{"metadata":{"id":"KIZOoLTJNB_a","outputId":"12e2fe6a-2f7c-4f8f-9a2f-4b9817c6d033","trusted":true},"cell_type":"code","source":"sample_x = [\"The phone offers less features\"]\nsample_y = [1]\npreds,probs,accuracy = evaluate_thru_pipeline(bert_classifier,sample_x,sample_y)\nprint(probs)\nprint(\"sentiment is: \",preds)","execution_count":null,"outputs":[]},{"metadata":{"id":"lJuifmc5jUIG","trusted":true},"cell_type":"code","source":"def evaluate_roc(probs, y_true):\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"R5koqyzY49C6","trusted":true},"cell_type":"code","source":"# from sklearn.metrics import classification_report,confusion_matrix\n# preds = probs[:, 1]\n# y_pred = np.where(preds >= 0.5, 1, 0)\n# print(classification_report(y_val, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"id":"MsMYVsOi9_t4","trusted":true},"cell_type":"code","source":"\"\"\"\nmarked_text = \"[CLS] \" + text + \" [SEP]\"\ntokenized_text = tokenizer.tokenize(marked_text)\nindexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)  ## here ids are the indices in vocabulary of Bert\nsegment_ids = [1] * len(tokenized_text)\ntokens_tensor = torch.tensor([indexed_tokens])\nsegments_tensor = torch.tensor([segment_ids])\n\"\"\"","execution_count":null,"outputs":[]},{"metadata":{"id":"JCrx6BbL_FLq","trusted":true},"cell_type":"code","source":"# BERT.eval()\n# with torch.no_grad():\n#     outputs = BERT(tokens_tensor,segments_tensor)\n#     hidden_states = outputs.hidden_states\n# token_embeddings = torch.stack(hidden_states,dim = 0)\n# token_embeddings = torch.squeeze(token_embeddings,dim = 1)\n# token_embeddings = token_embeddings.permute(1,0,2)\n# token_embeddings.size()","execution_count":null,"outputs":[]},{"metadata":{"id":"lP4zgV1kBsmW","trusted":true},"cell_type":"code","source":"# # Stores the token vectors, with shape [22 x 768]\n# token_vecs_sum = []\n\n# # `token_embeddings` is a [22 x 12 x 768] tensor.\n\n# # For each token in the sentence...\n# for token in token_embeddings:\n\n#     # `token` is a [12 x 768] tensor\n\n#     # Sum the vectors from the last four layers.\n#     sum_vec = torch.sum(token[-4:], dim=0)\n    \n#     # Use `sum_vec` to represent `token`.\n#     token_vecs_sum.append(sum_vec)","execution_count":null,"outputs":[]},{"metadata":{"id":"brfWOCvhCKL8","trusted":true},"cell_type":"code","source":"# idx1 = tokenized_text.index(\"apple\")\n# idx2 = tokenized_text.index(\"samsung\")\n# print(idx1,idx2)","execution_count":null,"outputs":[]},{"metadata":{"id":"9ZR6zR8oDZvx","trusted":true},"cell_type":"code","source":"# Display the words with their indeces.\n# for tup in zip(tokenized_text, indexed_tokens):\n#     print('{:<12} {:>6,}'.format(tup[0], tup[1]))","execution_count":null,"outputs":[]},{"metadata":{"id":"HOseM6htHOha","trusted":true},"cell_type":"code","source":"# context_vec1 = token_vecs_sum[idx1]\n# context_vec2 = token_vecs_sum[idx2]\n# print(context_vec1.size(),context_vec2.size())","execution_count":null,"outputs":[]},{"metadata":{"id":"4bWttdtrH420","trusted":true},"cell_type":"code","source":"# cos = nn.CosineSimilarity(dim=0)\n# print(cos(context_vec1,context_vec2))","execution_count":null,"outputs":[]},{"metadata":{"id":"95AH7h9PIgAm","trusted":true},"cell_type":"code","source":"# attention_text = outputs.attentions\n# attn = torch.stack(attention_text,dim = 0)\n# attn = torch.squeeze(attn,dim = 1)\n# print(attn.size())","execution_count":null,"outputs":[]},{"metadata":{"id":"iZZ8_bGMA1wC","trusted":true},"cell_type":"code","source":"# plt.figure(figsize = (25,25))\n# sbn.heatmap(data = attn[0][0],\n#             xticklabels = tokenized_text,\n#             yticklabels = tokenized_text)\n\n# plt.show()","execution_count":null,"outputs":[]},{"metadata":{"id":"VVLMwEU-Eb2q","trusted":true},"cell_type":"code","source":"# plt.plot(attn[3][11][idx1])\n# plt.show()\n# print(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"id":"o2NwdjEyGWq6","trusted":true},"cell_type":"code","source":"# plt.plot(attn[5][11][idx2])\n# plt.show()\n# print(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"id":"sERu9tnRLy05","trusted":true},"cell_type":"code","source":"# tot = torch.mean(attn,dim = 0)\n# tot = torch.mean(tot,dim = 0)\n# print(tot.size())","execution_count":null,"outputs":[]},{"metadata":{"id":"d3VYYOSyMLPS","trusted":true},"cell_type":"code","source":"# plt.plot(tot[idx1])\n# plt.show()\n# print(tokenized_text)","execution_count":null,"outputs":[]},{"metadata":{"id":"RdHu1qh6psIi","trusted":true},"cell_type":"code","source":"# logits = outputs.pooler_output\n# probs = F.softmax(logits,dim = 1)\n# print(probs.size())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}