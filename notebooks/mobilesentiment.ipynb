{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_1.csv\n/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_5.csv\n/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_2.csv\n/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_4.csv\n/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_3.csv\n/kaggle/input/14-million-cell-phone-reviews/phone_user_review_file_6.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = ['../input/14-million-cell-phone-reviews/phone_user_review_file_1.csv','../input/14-million-cell-phone-reviews/phone_user_review_file_2.csv',\n              '../input/14-million-cell-phone-reviews/phone_user_review_file_3.csv','../input/14-million-cell-phone-reviews/phone_user_review_file_4.csv',\n              '../input/14-million-cell-phone-reviews/phone_user_review_file_5.csv','../input/14-million-cell-phone-reviews/phone_user_review_file_6.csv']","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_data(path):\n    x = pd.read_csv(path,engine='python')\n    x = x[x['lang'] == 'en']\n    x.reset_index(inplace=True)\n    return x[['score','extract','product']]\ndata = extract_data('../input/14-million-cell-phone-reviews/phone_user_review_file_3.csv')","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.dropna(inplace=True)","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(data.score)","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"array([ 1. ,  2. ,  2.4,  2.8,  3. ,  3.2,  3.6,  4. ,  4.4,  4.8,  5. ,\n        5.2,  5.6,  6. ,  6.4,  6.8,  7. ,  7.1,  7.2,  7.3,  7.4,  7.5,\n        7.6,  7.7,  7.8,  7.9,  8. ,  8.3,  8.4,  8.6,  8.8,  9. ,  9.2,\n        9.3,  9.6, 10. ])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def apply_trans(p):\n    if p >= 4. and p <= 6.:\n        return 1\n    elif p < 4.:\n        return 0\n    else:\n        return 2\n        \ndata['score'] = data['score'].apply(lambda x :apply_trans(x))","execution_count":80,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.extract.values\nY = data.score.values","execution_count":83,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","execution_count":98,"outputs":[{"output_type":"stream","text":"There are 1 GPU(s) available.\nDevice name: Tesla P100-PCIE-16GB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 300","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ndef preprocessing_for_bert(data):\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        encoded_sent = tokenizer.encode_plus(\n            text=sent,  \n            add_special_tokens=True,        \n            max_length=MAX_LEN,             \n            pad_to_max_length=True,         \n            #return_tensors='pt',           \n            return_attention_mask=True   \n            )\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","execution_count":85,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"533851bd5f6d46809d10e06547ceaa5c"}},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val =\\\n    train_test_split(X, Y, test_size=0.1, random_state=2021)","execution_count":88,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn","execution_count":90,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","execution_count":91,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_inputs.shape","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"torch.Size([127808, 300])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 80, 3\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            #nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","execution_count":94,"outputs":[{"output_type":"stream","text":"CPU times: user 3 Âµs, sys: 0 ns, total: 3 Âµs\nWall time: 5.72 Âµs\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=1e-4,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss / len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","execution_count":96,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_seed(42)   \nbert_classifier, optimizer, scheduler = initialize_model(epochs=1)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)","execution_count":99,"outputs":[{"output_type":"stream","text":"Start training...\n\n Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n----------------------------------------------------------------------\n   1    |   20    |   0.742062   |     -      |     -     |   20.61  \n   1    |   40    |   0.569347   |     -      |     -     |   19.01  \n   1    |   60    |   0.532064   |     -      |     -     |   18.98  \n   1    |   80    |   0.522825   |     -      |     -     |   19.00  \n   1    |   100   |   0.528502   |     -      |     -     |   18.99  \n   1    |   120   |   0.532949   |     -      |     -     |   19.00  \n   1    |   140   |   0.453823   |     -      |     -     |   19.02  \n   1    |   160   |   0.523114   |     -      |     -     |   18.99  \n   1    |   180   |   0.489745   |     -      |     -     |   19.02  \n   1    |   200   |   0.564769   |     -      |     -     |   18.98  \n   1    |   220   |   0.532470   |     -      |     -     |   19.01  \n   1    |   240   |   0.528589   |     -      |     -     |   19.04  \n   1    |   260   |   0.408570   |     -      |     -     |   19.07  \n   1    |   280   |   0.478827   |     -      |     -     |   19.00  \n   1    |   300   |   0.465592   |     -      |     -     |   18.98  \n   1    |   320   |   0.471927   |     -      |     -     |   18.98  \n   1    |   340   |   0.514135   |     -      |     -     |   19.00  \n   1    |   360   |   0.532366   |     -      |     -     |   18.99  \n   1    |   380   |   0.484244   |     -      |     -     |   19.00  \n   1    |   400   |   0.515666   |     -      |     -     |   19.01  \n   1    |   420   |   0.505774   |     -      |     -     |   19.06  \n   1    |   440   |   0.521062   |     -      |     -     |   18.98  \n   1    |   460   |   0.455171   |     -      |     -     |   19.00  \n   1    |   480   |   0.461411   |     -      |     -     |   19.06  \n   1    |   500   |   0.482493   |     -      |     -     |   19.00  \n   1    |   520   |   0.458911   |     -      |     -     |   19.02  \n   1    |   540   |   0.468401   |     -      |     -     |   18.99  \n   1    |   560   |   0.498028   |     -      |     -     |   19.03  \n   1    |   580   |   0.505455   |     -      |     -     |   19.04  \n   1    |   600   |   0.442606   |     -      |     -     |   19.00  \n   1    |   620   |   0.434556   |     -      |     -     |   19.01  \n   1    |   640   |   0.446180   |     -      |     -     |   19.01  \n   1    |   660   |   0.483627   |     -      |     -     |   19.06  \n   1    |   680   |   0.474060   |     -      |     -     |   19.18  \n   1    |   700   |   0.477677   |     -      |     -     |   18.99  \n   1    |   720   |   0.544283   |     -      |     -     |   19.03  \n   1    |   740   |   0.442677   |     -      |     -     |   19.01  \n   1    |   760   |   0.514003   |     -      |     -     |   19.04  \n   1    |   780   |   0.428195   |     -      |     -     |   19.05  \n   1    |   800   |   0.477098   |     -      |     -     |   19.04  \n   1    |   820   |   0.436375   |     -      |     -     |   19.00  \n   1    |   840   |   0.478635   |     -      |     -     |   18.96  \n   1    |   860   |   0.428277   |     -      |     -     |   19.05  \n   1    |   880   |   0.441421   |     -      |     -     |   19.03  \n   1    |   900   |   0.434145   |     -      |     -     |   19.00  \n   1    |   920   |   0.462594   |     -      |     -     |   18.98  \n   1    |   940   |   0.441916   |     -      |     -     |   19.04  \n   1    |   960   |   0.476751   |     -      |     -     |   19.03  \n   1    |   980   |   0.427027   |     -      |     -     |   18.98  \n   1    |  1000   |   0.446401   |     -      |     -     |   19.02  \n   1    |  1020   |   0.478951   |     -      |     -     |   19.03  \n   1    |  1040   |   0.434872   |     -      |     -     |   19.02  \n   1    |  1060   |   0.467409   |     -      |     -     |   19.01  \n   1    |  1080   |   0.435873   |     -      |     -     |   19.00  \n   1    |  1100   |   0.424448   |     -      |     -     |   19.03  \n   1    |  1120   |   0.471247   |     -      |     -     |   19.13  \n   1    |  1140   |   0.422621   |     -      |     -     |   18.98  \n   1    |  1160   |   0.499398   |     -      |     -     |   19.07  \n   1    |  1180   |   0.466349   |     -      |     -     |   18.99  \n   1    |  1200   |   0.480916   |     -      |     -     |   19.02  \n   1    |  1220   |   0.404522   |     -      |     -     |   19.02  \n   1    |  1240   |   0.422346   |     -      |     -     |   18.98  \n   1    |  1260   |   0.435011   |     -      |     -     |   19.02  \n   1    |  1280   |   0.495690   |     -      |     -     |   18.98  \n   1    |  1300   |   0.431500   |     -      |     -     |   19.07  \n   1    |  1320   |   0.466316   |     -      |     -     |   19.00  \n   1    |  1340   |   0.458460   |     -      |     -     |   19.02  \n   1    |  1360   |   0.484647   |     -      |     -     |   19.00  \n   1    |  1380   |   0.445264   |     -      |     -     |   19.03  \n   1    |  1400   |   0.443386   |     -      |     -     |   19.04  \n   1    |  1420   |   0.418010   |     -      |     -     |   19.05  \n   1    |  1440   |   0.384915   |     -      |     -     |   19.03  \n   1    |  1460   |   0.459079   |     -      |     -     |   19.03  \n   1    |  1480   |   0.438223   |     -      |     -     |   18.97  \n   1    |  1500   |   0.430953   |     -      |     -     |   19.00  \n   1    |  1520   |   0.389944   |     -      |     -     |   18.98  \n   1    |  1540   |   0.469744   |     -      |     -     |   19.08  \n   1    |  1560   |   0.403175   |     -      |     -     |   19.10  \n   1    |  1580   |   0.471319   |     -      |     -     |   19.00  \n   1    |  1600   |   0.452275   |     -      |     -     |   19.01  \n   1    |  1620   |   0.493964   |     -      |     -     |   19.00  \n   1    |  1640   |   0.480901   |     -      |     -     |   19.06  \n   1    |  1660   |   0.384180   |     -      |     -     |   19.02  \n   1    |  1680   |   0.443143   |     -      |     -     |   19.00  \n   1    |  1700   |   0.431225   |     -      |     -     |   19.04  \n   1    |  1720   |   0.434063   |     -      |     -     |   19.03  \n   1    |  1740   |   0.435999   |     -      |     -     |   19.04  \n   1    |  1760   |   0.449306   |     -      |     -     |   19.05  \n   1    |  1780   |   0.407900   |     -      |     -     |   19.01  \n   1    |  1800   |   0.429285   |     -      |     -     |   19.03  \n   1    |  1820   |   0.448351   |     -      |     -     |   19.00  \n   1    |  1840   |   0.424752   |     -      |     -     |   19.10  \n   1    |  1860   |   0.395022   |     -      |     -     |   19.04  \n   1    |  1880   |   0.413134   |     -      |     -     |   19.01  \n   1    |  1900   |   0.470842   |     -      |     -     |   19.02  \n   1    |  1920   |   0.510329   |     -      |     -     |   18.99  \n   1    |  1940   |   0.450987   |     -      |     -     |   19.04  \n   1    |  1960   |   0.427200   |     -      |     -     |   18.99  \n   1    |  1980   |   0.410688   |     -      |     -     |   18.97  \n   1    |  2000   |   0.399506   |     -      |     -     |   19.04  \n   1    |  2020   |   0.423123   |     -      |     -     |   18.96  \n   1    |  2040   |   0.404013   |     -      |     -     |   19.05  \n   1    |  2060   |   0.426303   |     -      |     -     |   19.02  \n   1    |  2080   |   0.438926   |     -      |     -     |   19.05  \n   1    |  2100   |   0.403957   |     -      |     -     |   19.04  \n   1    |  2120   |   0.445556   |     -      |     -     |   18.99  \n   1    |  2140   |   0.399615   |     -      |     -     |   19.02  \n   1    |  2160   |   0.459053   |     -      |     -     |   19.01  \n   1    |  2180   |   0.433943   |     -      |     -     |   19.02  \n   1    |  2200   |   0.385653   |     -      |     -     |   19.08  \n   1    |  2220   |   0.426306   |     -      |     -     |   19.01  \n   1    |  2240   |   0.406929   |     -      |     -     |   19.05  \n   1    |  2260   |   0.420500   |     -      |     -     |   19.02  \n   1    |  2280   |   0.400880   |     -      |     -     |   19.07  \n   1    |  2300   |   0.446609   |     -      |     -     |   19.03  \n","name":"stdout"},{"output_type":"stream","text":"   1    |  2320   |   0.450012   |     -      |     -     |   19.00  \n   1    |  2340   |   0.405264   |     -      |     -     |   18.98  \n   1    |  2360   |   0.445745   |     -      |     -     |   19.00  \n   1    |  2380   |   0.474855   |     -      |     -     |   19.02  \n   1    |  2400   |   0.376611   |     -      |     -     |   19.00  \n   1    |  2420   |   0.443840   |     -      |     -     |   19.02  \n   1    |  2440   |   0.435626   |     -      |     -     |   19.05  \n   1    |  2460   |   0.446441   |     -      |     -     |   18.99  \n   1    |  2480   |   0.401807   |     -      |     -     |   19.02  \n   1    |  2500   |   0.411928   |     -      |     -     |   19.02  \n   1    |  2520   |   0.422835   |     -      |     -     |   18.97  \n   1    |  2540   |   0.395509   |     -      |     -     |   19.03  \n   1    |  2560   |   0.405557   |     -      |     -     |   18.99  \n   1    |  2580   |   0.432078   |     -      |     -     |   19.01  \n   1    |  2600   |   0.388235   |     -      |     -     |   19.02  \n   1    |  2620   |   0.398328   |     -      |     -     |   18.98  \n   1    |  2640   |   0.431200   |     -      |     -     |   19.00  \n   1    |  2660   |   0.417487   |     -      |     -     |   19.01  \n   1    |  2680   |   0.413781   |     -      |     -     |   19.06  \n   1    |  2700   |   0.427769   |     -      |     -     |   19.00  \n   1    |  2720   |   0.403897   |     -      |     -     |   19.00  \n   1    |  2740   |   0.450269   |     -      |     -     |   19.00  \n   1    |  2760   |   0.386155   |     -      |     -     |   18.99  \n   1    |  2780   |   0.414384   |     -      |     -     |   19.06  \n   1    |  2800   |   0.425902   |     -      |     -     |   19.04  \n   1    |  2820   |   0.437877   |     -      |     -     |   18.98  \n   1    |  2840   |   0.400912   |     -      |     -     |   18.99  \n   1    |  2860   |   0.425236   |     -      |     -     |   19.02  \n   1    |  2880   |   0.416786   |     -      |     -     |   19.02  \n   1    |  2900   |   0.428351   |     -      |     -     |   19.01  \n   1    |  2920   |   0.407557   |     -      |     -     |   18.99  \n   1    |  2940   |   0.351952   |     -      |     -     |   19.06  \n   1    |  2960   |   0.414577   |     -      |     -     |   19.00  \n   1    |  2980   |   0.392150   |     -      |     -     |   19.02  \n   1    |  3000   |   0.403147   |     -      |     -     |   18.99  \n   1    |  3020   |   0.351346   |     -      |     -     |   19.07  \n   1    |  3040   |   0.392701   |     -      |     -     |   19.00  \n   1    |  3060   |   0.373478   |     -      |     -     |   18.97  \n   1    |  3080   |   0.386157   |     -      |     -     |   19.02  \n   1    |  3100   |   0.309442   |     -      |     -     |   19.01  \n   1    |  3120   |   0.387140   |     -      |     -     |   19.02  \n   1    |  3140   |   0.447320   |     -      |     -     |   19.00  \n   1    |  3160   |   0.404258   |     -      |     -     |   18.99  \n   1    |  3180   |   0.443140   |     -      |     -     |   19.02  \n   1    |  3200   |   0.447487   |     -      |     -     |   18.96  \n   1    |  3220   |   0.413114   |     -      |     -     |   19.01  \n   1    |  3240   |   0.373387   |     -      |     -     |   19.07  \n   1    |  3260   |   0.418318   |     -      |     -     |   19.01  \n   1    |  3280   |   0.382067   |     -      |     -     |   19.02  \n   1    |  3300   |   0.389005   |     -      |     -     |   19.02  \n   1    |  3320   |   0.398733   |     -      |     -     |   19.02  \n   1    |  3340   |   0.387138   |     -      |     -     |   19.10  \n   1    |  3360   |   0.418033   |     -      |     -     |   19.05  \n   1    |  3380   |   0.454508   |     -      |     -     |   19.05  \n   1    |  3400   |   0.407984   |     -      |     -     |   18.99  \n   1    |  3420   |   0.391616   |     -      |     -     |   19.02  \n   1    |  3440   |   0.432256   |     -      |     -     |   19.03  \n   1    |  3460   |   0.396064   |     -      |     -     |   18.98  \n   1    |  3480   |   0.348118   |     -      |     -     |   19.02  \n   1    |  3500   |   0.433994   |     -      |     -     |   18.98  \n   1    |  3520   |   0.367682   |     -      |     -     |   19.03  \n   1    |  3540   |   0.351862   |     -      |     -     |   18.99  \n   1    |  3560   |   0.427811   |     -      |     -     |   19.04  \n   1    |  3580   |   0.403307   |     -      |     -     |   19.05  \n   1    |  3600   |   0.350253   |     -      |     -     |   19.03  \n   1    |  3620   |   0.391632   |     -      |     -     |   19.07  \n   1    |  3640   |   0.428255   |     -      |     -     |   19.04  \n   1    |  3660   |   0.392589   |     -      |     -     |   19.00  \n   1    |  3680   |   0.358597   |     -      |     -     |   19.04  \n   1    |  3700   |   0.422817   |     -      |     -     |   19.03  \n   1    |  3720   |   0.374079   |     -      |     -     |   19.03  \n   1    |  3740   |   0.426535   |     -      |     -     |   19.09  \n   1    |  3760   |   0.421079   |     -      |     -     |   19.00  \n   1    |  3780   |   0.390296   |     -      |     -     |   19.04  \n   1    |  3800   |   0.447420   |     -      |     -     |   18.98  \n   1    |  3820   |   0.423110   |     -      |     -     |   19.03  \n   1    |  3840   |   0.352542   |     -      |     -     |   18.99  \n   1    |  3860   |   0.349277   |     -      |     -     |   19.03  \n   1    |  3880   |   0.387511   |     -      |     -     |   18.99  \n   1    |  3900   |   0.374379   |     -      |     -     |   19.04  \n   1    |  3920   |   0.388895   |     -      |     -     |   19.04  \n   1    |  3940   |   0.357336   |     -      |     -     |   18.97  \n   1    |  3960   |   0.410691   |     -      |     -     |   19.02  \n   1    |  3980   |   0.392223   |     -      |     -     |   19.03  \n   1    |  3993   |   0.412525   |     -      |     -     |   12.36  \n----------------------------------------------------------------------\n   1    |    -    |   0.436165   |  0.391390  |   83.87   |  3938.95 \n----------------------------------------------------------------------\n\n\nTraining complete!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    model.eval()\n\n    all_logits = []\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = bert_predict(bert_classifier, val_dataloader)","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred = np.argmax(probs,axis=1)","execution_count":102,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_val,pred))","execution_count":103,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n           0       0.73      0.76      0.74      2346\n           1       0.58      0.43      0.49      2264\n           2       0.91      0.96      0.93      9591\n\n    accuracy                           0.84     14201\n   macro avg       0.74      0.71      0.72     14201\nweighted avg       0.83      0.84      0.83     14201\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f1_score(y_val,pred,average='weighted'))","execution_count":104,"outputs":[{"output_type":"stream","text":"0.8302402272033141\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(confusion_matrix(y_val,pred))","execution_count":105,"outputs":[{"output_type":"stream","text":"[[1784  402  160]\n [ 550  965  749]\n [ 122  308 9161]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv = pd.read_pickle('../input/sentimenttrans/deta.pkl')\ntest_csv","execution_count":106,"outputs":[{"output_type":"execute_result","execution_count":106,"data":{"text/plain":"                                                  Text\n0    OPPO K7X launched with 5G support and quad-rea...\n1                      February 7, 2021 | 1: 46 | IST \n2    Motorola can launch two new smartphones - Moto...\n3    Nokia 5. 4 features a 6. 39-inch HD+ IPS LCD d...\n4    Buy Amazon Cell 2020 within 7000 RS These attr...\n..                                                 ...\n749  Vivo S9 could launch on March 6, as per a tips...\n750  Samsung Your Next Flagship i.e. Galaxy S30 Ser...\n751  Poco launched the Poco M3 smartphone in India ...\n752  Great Indian Festival Sale is getting started ...\n753  Motorola is working on its Affordable 5G smart...\n\n[754 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OPPO K7X launched with 5G support and quad-rea...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>February 7, 2021 | 1: 46 | IST</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Motorola can launch two new smartphones - Moto...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Nokia 5. 4 features a 6. 39-inch HD+ IPS LCD d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Buy Amazon Cell 2020 within 7000 RS These attr...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>Vivo S9 could launch on March 6, as per a tips...</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>Samsung Your Next Flagship i.e. Galaxy S30 Ser...</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>Poco launched the Poco M3 smartphone in India ...</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>Great Indian Festival Sale is getting started ...</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>Motorola is working on its Affordable 5G smart...</td>\n    </tr>\n  </tbody>\n</table>\n<p>754 rows Ã 1 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_inputs, test_masks = preprocessing_for_bert(test_csv.Text)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_dataloader = DataLoader(test_dataset,batch_size=32)","execution_count":107,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  FutureWarning,\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"probs = bert_predict(bert_classifier, test_dataloader)\npred = np.argmax(probs,axis=1)\ntest_csv.loc[:,'label'] = pred\nrevdic = {1:'neutral' , 0:'negative',2:'positive'}\ntest_csv.loc[:,'sentiment'] = test_csv.label.map(revdic)","execution_count":108,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv","execution_count":109,"outputs":[{"output_type":"execute_result","execution_count":109,"data":{"text/plain":"                                                  Text  label sentiment\n0    OPPO K7X launched with 5G support and quad-rea...      2  positive\n1                      February 7, 2021 | 1: 46 | IST       2  positive\n2    Motorola can launch two new smartphones - Moto...      2  positive\n3    Nokia 5. 4 features a 6. 39-inch HD+ IPS LCD d...      2  positive\n4    Buy Amazon Cell 2020 within 7000 RS These attr...      2  positive\n..                                                 ...    ...       ...\n749  Vivo S9 could launch on March 6, as per a tips...      2  positive\n750  Samsung Your Next Flagship i.e. Galaxy S30 Ser...      2  positive\n751  Poco launched the Poco M3 smartphone in India ...      2  positive\n752  Great Indian Festival Sale is getting started ...      2  positive\n753  Motorola is working on its Affordable 5G smart...      2  positive\n\n[754 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n      <th>label</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>OPPO K7X launched with 5G support and quad-rea...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>February 7, 2021 | 1: 46 | IST</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Motorola can launch two new smartphones - Moto...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Nokia 5. 4 features a 6. 39-inch HD+ IPS LCD d...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Buy Amazon Cell 2020 within 7000 RS These attr...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>749</th>\n      <td>Vivo S9 could launch on March 6, as per a tips...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>Samsung Your Next Flagship i.e. Galaxy S30 Ser...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>751</th>\n      <td>Poco launched the Poco M3 smartphone in India ...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>752</th>\n      <td>Great Indian Festival Sale is getting started ...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>753</th>\n      <td>Motorola is working on its Affordable 5G smart...</td>\n      <td>2</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n<p>754 rows Ã 3 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.unique(test_csv.label)","execution_count":110,"outputs":[{"output_type":"execute_result","execution_count":110,"data":{"text/plain":"array([0, 1, 2])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_csv.to_csv('trans.csv')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}